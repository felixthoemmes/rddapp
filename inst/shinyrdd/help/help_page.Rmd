---
title: "`rddapp` `shiny` manual"
author: "Felix Thoemmes"
#date: "January 20, 2018"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: "cosmo"
    css: help_page.css
---

`rddapp` is an [`R` package](https://cran.r-project.org/web/packages/rddapp/index.html) with a `shiny` [web app](https://shiny.rstudio.com/) for the analysis of [regression discontinuity designs](https://en.wikipedia.org/wiki/Regression_discontinuity_design) (RDDs). It offers both parametric and nonparametric estimation of causal effects in RDDs, with one or two assignment variables. It also provides numerous assumption checks, and allows the estimation of statistical power.

# Use

## Online
The `rddapp` `shiny` web app can be accessed [online](https://rddapp.shinyapps.io/shinyrdd/). Untested features will first be introduced to the beta [version](https://rddapp.shinyapps.io/shinyrdd_beta/).
The `shiny` environment is fully point-and-click; no programming in `R` is required. Data can be uploaded directly to the `shiny` server for analysis. For sensitive data that must be kept on secure, the use of an offline client is available.

## Offline
To use the `shiny` app offline, `R` must be [installed](https://cran.cnr.berkeley.edu/). Next, open an `R` session and download [the offical rddapp R package from CRAN](https://cran.r-project.org/web/packages/rddapp/index.html). Load the package using the `library(rddapp)` command in the `R` prompt, and then execute the function `shiny_run()`. This command will load the point-and-click `shiny` environment in a local browser. No further interaction with the `R` console is required and no data will be transferred over the Internet.

# Interface
At the very top of the `shiny` app are three menu options, labeled [**Model**](#model_page), [**Power**](#power_page), and **More**. The **Model** page is for statistical estimation, the **Power** page is for power analysis, and the **More** menu contains this manual, as well as a simple **About** page.

# Model page {#model_page}
The **Model** page is subdivided into a data panel and a dynamic results panel which appears after the data are loaded.

## Data upload panel
Data are uploaded using the pull-down menu in the **Data** panel. The choices are to upload either a `.sav` file (native file format of `SPSS`), or a `.csv` file (comma separated values).

The user can further fine-tune the data upload by clicking on the small **gear** button at the top right of the panel to reveal additional options depending on the file format chosen.

The **Browse** button appears automatically when the user chooses a `.sav` or `.csv` file and opens a file explorer from which to select the data for upload.

Once the data are loaded, additional panels will appear for the [**Outcome**](#outcome_panel) variable and [**Treatment design**](#treatment_design_panel).

### `.sav` files {#sav_files}

If the user selects an `.sav` file for upload, the **gear** button in the top right of the **Data** panel can be used to display further options.

If the **Use Value Labels** option is selected, value labels from SPSS will be used for the uploaded data. For instance, a user may have coded a categorical variable `0` and `1`, and included corresponding value labels `male` and `female`. When **Use Value Labels** is activated, the actual labels will be displayed; otherwise, the numeric codes will be used.

If the **Use User's Missing Value** option is selected, any user-specified missing values in `SPSS` will be treated as [`NA`](https://www.statmethods.net/input/missingdata.html) in the `R` program.

Finally, the user can indicate that the uploaded file has been multiply imputed (typically to deal with missing values) by selecting a **Multiple Imputation ID** variable from the data. This variable indicates which imputation event each observation corresponds to. All analyses will be performed on each imputation, estimates from each imputation will be averaged, and standard errors will be derived using Rubin's rules (1987), which accounts for the variation both within and between imputations.

### `.csv` files

If the user selects a `.csv` file, the options under the **gear** button allow the user to specify exactly how the `.csv` file is structured.

The user can specify:

* whether any values are enclosed in **quotes** (double `"`, single `'`, or none),
* what kind of **separator** is used between values (comma `,`, semi-colon `;`, tab, or whitespace),
* whether **missing values** are blank by default or if a special character is used (e.g., `NA`),
* whether the file includes a **header** in the first line with variable names. 

Just as for `.sav` files, the user can specify that the file is multiply imputed, and if so which variable is the **Multiple Imputation ID** (see [above](#sav_files) for details).

### CARE example data

The application includes an example data set, named **CARE**. The data are sampled from the Carolina Abecedarian Project and the Carolina Approach to Responsive Education (CARE) study, with the full data set hosted by [ICPSR](http://www.icpsr.umich.edu/icpsrweb/ICPSR/studies/4091). The example data are described briefly in the `rddapp` package [manual](https://cran.r-project.org/web/packages/rddapp/rddapp.pdf), and in detail in the [vignette](https://cran.r-project.org/web/packages/rddapp/vignettes/rddapp.html).

## Outcome panel {#outcome_panel}
The **Outcome** menu allows the user to define the outcome variable: the variable that the researcher believes is affected by the treatment. Additional options for analyzing the outcome variable appear by clicking the **gear** button and include the **Kernel for local linear fitting**, **Type of SE** (standard error), and **Auxiliary Variables**.

The **kernel** option specifies the type of kernel that should be used for the nonparametric (local linear fitting) analysis, described further in the [**Estimates**](#estimates_tab) section. By default, a triangular kernel is chosen, in line with recommendations of Imbens and Kalyanaraman (XXXX). However, the user can change this to a variety of other kernels, defined by the following functions:

  1. Triangular:   $K(u) = (1 - |u|)$,
  2. Rectangular (uniform):  $K(u) = \frac{1}{2}$,
  3. Epanechnikov: $K(u) = \frac{3}{4} (1 - u^2)$,
  4. Quartic (biweight):      $K(u) = \frac{15}{16} (1 - u^2)^2$,
  5. Triweight:    $K(u) = \frac{35}{32}(1 - u^2)^3$,
  6. Tricube:      $K(u) = \frac{70}{81}(1 - |u|^3)^3$,
  7. Gaussian:     $K(u) = \frac{1}{\sqrt{2\pi}} e^{-u^2/2}$,
  8. Cosine:       $K(u) = \frac{\pi}{4}\cos(\frac{\pi}{2}u)$,

where $|u|\leq 1$, except for with the Gaussian kernel, where $u$ can be any real number.

The second option under the outcome tab is the **type of standard error (SE)** that should be used. `rddapp` supports a "regular" standard error, that assumes constant variance ($\sigma^2 (X'X)^{-1}$), as well as all of the types of standard errors that are included in the `sandwich` package ([Zeileis, 2017](https://cran.r-project.org/web/packages/sandwich/sandwich.pdf)). By default, a [heteroskedasticty-robust standard error](https://en.wikipedia.org/wiki/Heteroscedasticity-consistent_standard_errors) (HC1) is chosen. If the user chooses the "One-way Cluster" option, an additional pull-down menu appears, prompting the user to select the **cluster ID** variable.

The **auxiliary variables** option allows users to specify the use of auxiliary variables in the RDD. These variables are integrated in the regressions of the parametric RDD. Purely linear terms are used for all auxiliary variables; there is currently no implementation for adding non-linear terms. To add non-linear terms, a user would have to create them by hand first (e.g., polynomials) and enter them as regular auxiliary variables. By default, the auxiliary variables will not be entered as interactive terms (i.e., interacting with the treatment assignment). In a properly executed RDD, it should not be necessary to include auxiliary variables, but in some instances, including them can increase precision of parameter estimates, and thus statistical power.

For some models, in particular the [frontier approach](#frontier_approach) for multiple assignment variables, an additional option will be displayed that enables the user to define the **number of bootstrap samples** that should be performed for estimation of standard errors and confidence intervals. 

## Treatment design panel {#treatment_design_panel}
The **treatment mechanism** of the RDD is a (series of) rules used to determine whether an individual is *selected* for treatment, but it does not necessarily imply that the individual *actually* received treatment. In a **sharp RDD**, all individuals that were assigned treatment actual received treatment. In a **fuzzy RDD**, not all individuals that were assigned the treatment received it and/or some individuals who were *not* assigned the treatment received it.

The mechanism is described by (a series) of "if" statements, in the **Treatment design** panel, under the heading **IF**. The user selects the **_assignment_ variable** in the drop-down menu labeled **primary assignment**. The box underneath is used to input the threshold for being assigned to the treatment. 
By default, every value of the assignment variable that is **greater than or equal to** the threshold leads to an assignment to the treatment condition. Clicking on the $\geq$ sign switches the condition to **less than or equal to** ($\leq$), meaning that values of the assignment variable less than or equal to the threshold lead to assignment. To exclude treatment at the threshold level, click the **gear** button and deselect the **Treatment at the cutoff(s)** option. This will convert the inequality to a **strict** greater than symbol ($>$) or less than symbol ($<$).

For example, choosing an assignment variable and then selecting greater than or equal to ($\geq$) with cut-off value 40, would mean that treatment was assigned to those individuals whose assignment variable was equal to or greater than 40. On the other hand, choosing strictly less than ($<$) a value of 10 would mean that individuals whose assignment variable was *strictly* less than 10 were assigned to the treatment.

Clicking the **plus** button in the top right corner of the **Treatment design** panel allows the user to select a **secondary assignment** variable, to accommodate **multiple-assignment RDDs**. Currently, the software only supports up to two assignment variables. One restriction that is in place for these designs is that *both* assignment rules must be either strict inequalities or not (i.e. either both $>$ and/or $<$ or both $\geq$ and/or $\leq$). <<!--IP: I'm not sure what  this sentence means... maybe try to clarify, specifically what "these two" is referring to: Combinations of these two lead to designs in which it is at least theoretically possible to have undefined cases for some analytic approaches. -->

Under the **Treatment Receipt** heading, the user selects the **_treatment_ variable** in pull-down menu labeled **actual treatment**. The treatment variable indicates whether the individual *actually* received treatment.

## Data analysis tab
The **Data** tab displays a preview of the data upon upload, mainly for the purposes of validating that the data were read into the program correctly, in the **Raw data** panel. Variable names are displayed as table column headers. Users can sort observations by each of the variables by either clicking the **up arrow** next to the variable name for **ascending order** or the **down arrow** for **descending order**. 

Upon specifying the [**Outcome** ](#outcome_panel) variable and the [**Treatment design**](#treatment_design_panel), a  **Model Data** button appears at the top right corner of the **Raw data** panel. Clicking this button restricts the preview table to just the outcome and treatment design variables.

Specifying the [**Outcome** ](#outcome_panel) variable and the [**Treatment design**](#treatment_design_panel) also generates two new result tables.

### Table 1.1

Table 1.1 shows descriptive statistics of the treatment variable (T), the assignment variable (A1 - and A2, if a second assignment was chosen), and the outcome variable (O). For each variable, three columns are reported: the sample size (**N**), the mean (**M**), and the standard deviation (**SD**). In addition, a correlation matrix is shown on the side, whose columns are treatment (**T**) and outcome (**O**), and whose values are the Pearson correlation coefficient between variables. Clicking on the **notepad** icon in the top right corner of the Table 1.1 panel downloads the data as a `.csv` file.

### Table 1.2

Table 1.2 summarizes the design of the RDD, and can serve to validate that the design was correctly specified in the [**Treatment design**](#treatment_design) panel. Entries of this table are sample sizes, **n**. For an RDD with a single **assignment variable**, rows are the two different groups given by the treatment design (i.e. having the assignment variable below or above the specified threshold). The columns specify whether the treatment was *actually* received within each group, based on the specified **treatment receipt** variable.

In a **sharp RDD** (a design in which assigned and received treatment always coincide), the off-diagonal cell counts are always zero. In a **fuzzy RDD**, the off-diagonal elements will contain sample sizes of incorrectly classified individuals (i.e. treatment assignment and received treatment do not coincide). In this case, the probability of receiving treatment, $\pi$, conditional on being on either side of the threshold is also displayed. This quantity is computed as a simple conditional probability, $P(T=1|A1)$; no parametric model is being used for this estimate.

If a **second assignment variable** is specified, an additional column is added for this variable. All combinations of cut-offs are displayed as single rows, with their respective sample sizes, **n**, and the estimated $\pi$ parameter.

`rddapp` reports further details of the treatment design under the design summary table. It determines whether the design is sharp or fuzzy, and whether one or two assignment variables were used. If the second assignment variable does not alter any of the treatments assigned based on the first assignment variable, the software notes that the second assignment variable is redundant, and calls this an **ineffective assignment**.

## Assumptions tab
RDDs rely on several assumptions, which are summarized in the assumptions tab. Depending on whether the design is sharp or fuzzy, and relies on one or two assignment variables, the content of the assumption tab will vary slightly.

### Figure 2.1

Figure 2.1 shows the result of **McCrary's** (2008) **sorting test**, which relies on the following idea: in a proper RDD, there should be **no discontinuity at the threshold** for treatment if participants were neutrally selected based on the assignment rule. The presence of a discontinuity could imply that there was self-selection for or against the treatment by some participants or others involved.

As an example, suppose that students are put on the dean's list if their GPA is 3.75 or higher. Teachers who (either implicitly or explicitly) want to help their students may round up a grade that is very close to the cut-off. A teacher may grade the final exam of a student with a GPA of 3.74 more leniently, so that the student can obtain a final GPA of 3.75 and make it on the dean's list. If such behavior was wide-spread, we would find fewer scores of 3.74 and just below, and more scores of 3.75 and just above. This effect would manifest itself as a **small discontinuity** in the distribution of GPA, right at the cut-off for treatment.

### McCrary's sorting test

The sorting test is implemented as described by McCrary (2008). The user specifies the assignment variable and threshold of interest in the pull-down menu labeled **Assignment [Cutoff]**. The data are aggregated with a default bin size and a nonparametric kernel smoother is applied to both sides of the cut-off using the midpoints of bins and the observed count in each bin as data. The test measures the vertical distance of the two edges on both sides of the cut-off with a log metric. 

Because the sorting test relies on a nonparametric estimation of the kernel density estimate, it is possible to change parameters of the test. Any changes to the parameters automatically update both the plot and the [test summary](#test_summary).

### Bin size

The **bin size** determines the width of bins for the kernel density estimate in the sorting test, which is performed on the means of each bin of raw data. As the bin size **decreases**, the number of bins with frequency at or close to 1 will increase, and so the density estimate will look **very flat** (near 0). As the bin size **increases**, the number of bins decreases, and the kernel density estimate will tend to be **under-smoothed** if it relies on too few bins.

### Band width

The **bandwidth**, also known as the smoothing parameter, determines how many data points are considered at once when computing the smooth approximation to the data. As the bandwidth **increases**, more data points are considered and so the smoother goes **from flat to jagged**. The choice of bandwidth will affect the test statistic, and possibly different inferential decisions. Generally speaking, a bandwidth parameter should be chosen so that **no over- or under-smoothing occurs**. To aid in the selection of the bandwidth parameter, Figure 2.1 will update every time the smoothing parameter is changed.

When changing the parameters, be mindful that it it is considered **bad statistical practice** to change the bin size or bandwidth to maximize the p-value in order to make the test look good. Instead either the default bin size should be used, or adjusted in accordance with a properly smoothed plot. The software defaults are often good choices, but sometimes minor adjustments are needed. <!-- IP: Citation for the default values and why they are good choices?-->

### Test summary {#test_summary}

The software derives a test statistic, **theta**, which quantifies the vertical distance, and a standard error (**se**). These quantities are tested using a $z$-test. The **z** value and the **p** value are also reported. Ideally, this test should *not* be significant. A significant test statistic would indicate that the assumption of a neutrally implemented RDD (assignment only due to cut-off) has been violated. The **test summary** is displayed in the lower right corner of the Figure 2.1 panel.

Both the plot and the test summary can be downloaded using the buttons in the top right corner of the Figure 2.1 panel. The plot can be downloaded as a `.png`, `.svg`, or `.pdf`. The test summary table can be downloaded as a `.csv` file. 

### Table 2.1

Table 2.1 displays the results of an attrition analysis. In each row, it provides a count, **N**, of missing data and corresponding percentages of the total sample size for each variable (treatment, outcome, and assignment). The three columns give these counts and percentages:

1. for **all** individuals (**Overall**);
2. restricted to individuals in the control group (**Control**);
3. restricted to individuals who received treatment (**Treatment**).
<!-- IP: double check that "control" and "treatment" in the software are decided by the treatment variable and not the assignment variable. -->

This information can help to determine whether any differential attrition occurred. The table can be downloaded as a `.csv` file using the button in the top right corner of the Table 2.1 panel.

## Estimates tab {#estimates_tab}

The estimates tab is where the user will find point estimates and inferential statistics for the treatment effect. Because the program allows for a variety of model specifications, the output can get quite lengthy.

### Table 3.1 & Figure 3.1

Table 3.1. contains **all results**, **treatment effect estimates**, and **inferential statistics**. The table updates automatically depending on the particular [treatment design](#treatment_design) chosen. The user can expand or reduce the amount of information that is being displayed using several toggle switches at the top right of the table, including those for **Parametric** (global) and **Nonparametric** (local) models, as well as **ITT**.<!-- IP: "ITT" should to be explained here.--> The table can be downloaded as a `.csv` file using the button in the top right corner of the Table 3.1 panel.

Figure 3.1 provides a visual display of the treatment effects described in Table 3.1.<!--IP: add more about plot options here--> The plot can be downloaded as a `.png`, `.svg`, and a `.pdf` using the buttons in the top right of the Figure 3.1 panel.

Because both Table 3.1 and Figure 3.1 automatically adjust based on the specified treatment design, we divide the following description of the table and figure into separate sections for each of the four different possible designs (sharp/fuzzy with single/multiple assignment rule).

#### Sharp RDD with a single assignment rule
In the case of **sharp** RDD with a **single** assignment rule, Table 3.1. displays the treatment effect estimates for both the parametric and nonparametric models.

The **parametric** model results are subdivided into three rows, where each row gives results of the outcome variable modeled as a function of the assignment variable, but with different functional forms in each case. The available functional forms are:

* **linear**, which only has a linear term;
* **quadratic**, which in addition to the linear term from above also has a quadratic term;
* **cubic**, which in addition to the linear and quadratic terms from above also has a cubic term.

Each term listed above has an associated regression coefficient. Moreover, these coefficients are allowed to differ before and after the assignment threshold. As a result, the model always includes an **interaction effect** between the assignment variable and the treatment status. <!-- IP: I don't quite understand what this last sentence means, so it might be worth trying to clarify. -->

<!-- IP: I'm trying to understand what the value in the "Est." column is for the parametric models. If each term has an associated regression coefficient as described above:
* is the single estimate reported in the table for each row just the coefficient of the highest order term?
* do the coefficients on the lower order terms change when we add higher order terms or can we also read these off the table from the previous rows?
* if these coefficients are allowed to vary before and after the assignment threshold, which single number is reported as the estimate for "the" coefficient? is it the difference between the coefficients before and after the treatment?

Should there be more guidance here about how to *interpret* these estimates? 
-->

By default, all parametric models used in the estimation of the treatment effect are performed using **two-stage least-squares** (2SLS) estimation, with **robust standard errors**. The first stage involves a model that uses the **assignment variable** to predict the **treatment variable**. Note that in sharp RDDs, this prediction of treatment assignment will *always* be perfect by definition, which renders this first step unnecessary; the whole process reduces down to a single-stage least-squares regression model. In the second stage, the predicted values from the first stage are used as predictors of the **outcome variable**. The treatment effect is captured in the regression coefficient that is associated with the **treatment  variable** in the *second stage* of estimation.

For each parametric model, several **inferential statistics** are reported in labeled columns:

* **Bandwidth**: empty for all parametric models because it only applies to nonparametric models;
* **N**: the total <u>sample size</u>;
* **Est.**: the <u>treatment effect estimate</u>;
* **SE**: the <u>standard error</u> associated with the treatment effect estimate;
* **z**: the <u>z-score</u> associated with the treatment effect estimate;
* **p**: the <u>p-value</u> for the estimate;
* **95% CI**: the <u>95% confidence interval</u> for the treatment effect estimate;
* **Cohen's d**: the <u>Cohen's d effect size</u>, which is computed as the mean difference at the cut-off divided by the standard deviation of all observed outcome values (same across all models to make direct comparisons of Cohen's d meaningful). 

The **nonparametric** models use a **weighted least-squares** linear regression. A bandwidth function determines how to weigh each data point in the regression, which determines both the **smoothness** of the regression, as well as the **window** around the treatment threshold from which to use data (points are excluded by being assigning a weight of 0).

The nonparametric model results are subdivided into three rows, where each row gives results of the outcome variable modeled as a function of the assignment variable, but with a different bandwidth function. The bandwidth functions include:

* **optimal**: the "optimal" bandwidth, as defined by Imbens and Kalyanaraman (XXXX);
* **half-optimal**: half the optimal bandwidth;
* **double-optimal**: double the optimal bandwidth.

Note that, unlike parametric models, these nonparametric models *differ in the sample size* being used in each regression (since the bandwidth function can assign 0 weight to some points).

The three different nonparametric models are run by default to allow the researcher to see whether results hold under a variety of bandwidths. Additional robustness checks are performed in the [sensitivities](#sensitivities_tab) tab.

<!-- IP: what does the ITT switch do? -->

If any covariates are specified in the **auxilliary variables** field found in the additional options of the [outcome](#outcome_panel) panel, an additional toggle button labeled **COV** is shown at the top right of the table. It is deselected by default, but if the user selects it, additional rows are added to Table 3.1 specifing the **regression coefficient** that describes the (linear) relationship between the covariate and the outcome is displayed, along with all **inferential statistics**. Note that even though in the table the regression coefficient is displayed under the same column heading (**Est.**), it does *not* mean that this is a treatment effect estimate. It is simply the linear relationship between covariate and the outcome variable. The regression coefficient for the covariate is obtained from the same model that is used to estimate the RDD treatment effect. That means, the treatment and assignment variables are *also* predictors in this model. It is currently not possible to model the relationship between the covariate and the outcome variable in a non-linear fashion (e.g., polynomial regression, interacted with the treatment, etc.).

<!-- IP: STOPPED EDITING HERE -->

Figure 3.1. right underneath the table with treatment effect estimates provides a visual display of the treatment effect. Every of the six models shown above can also be individually displayed, or overlaid on top of each other. By default the plot will show raw data values of the assignment variable, and the outcome variable in a scatterplot. With large datasets it may be unfeasible to display all raw data points. To correct for potential over-plotting, the user has the choice to bin the datapoints, using two options: evenly binning, and binning by quartiles. An even binning means that the assignment variable is sub-divided into a user-specified number of bins. In each of those bins, an average on the outcome variable is computed. This average is then plotted as a single point. The user has the choice to change the size of this point in proportion to the sample size in this bin, using the "scale point size" toggle button. In addition, the user may request a 95 % confidence interval around the estimate of the mean. These confidence intervals are simple normal-theory intervals around an observed mean, using the sample size of the number of units in the bin. The alternative binning option is to use quantiles of the assignment variable. Based on the user-specified number of bins, the assignment variable is cut along quantiles. This results in bins that are typically not the same length, but tend to have a more equal number of individuals in them. In order to avoid binning individuals that are on different sides of the cut-off, the binning algorithm will always avoid spanning a bin over the cut-off value. This can sometimes lead to slight distortions in the bin width and sample size within bins. Most noticeably this can lead to a situation in which quantile binning does not end up with an equal amount of individuals in each bin. 

The scatterplot is overlaid with the regression line that corresponds to one or more of the six possible models. By default, the nonparametric optimal model is overlaid. It is shown as a (smooth) regression line that covers all points around the cut-off that were included in the bandwidth (units with zero weights are not used in the construction of the regression line). By default, a 95% confidence band is displayed around the regression line. This band can be toggled on or off, to either show a dashed outline, or a filled area. The width of the confidence interval can be selected by the user, by overriding the value 95 with any other desired value. Clicking inside the box labeled "Predicted Lines" opens up a pull-down menu from which the user can select any model regression line (out of the six possible models) to be overlaid. A regression line can be deleted by clicking in the box and using the backspace button. 

Additional information of the graph (e.g., legends for points and regression lines, axis labels) are done automatically, and cannot be overriden by the user. The plot can be downloaded by clicking the buttons on the top right of the Figure. Supported formats are .PDF, .SVG, and .PNG. 

#### Fuzzy RDD with a single assignment rule
The fuzzy RDD with a single assignment variable is conceptually quite similar to the sharp RDD, and likewise the output in the estimates tab will be almost identical, and is therefore not repeated in detail. The fuzzy RDD estimates are derived using two-stage least-squares estimation. The first-stage model predicts treatment group status from the assignment variable, and in the second stage these predictions are used in a model with the outcome variable. Unlike in the case of the sharp RDD, the actual assigned treatment, and the treatment that is actually being received may be different. Hence, the predictions of treatment assignment from the assignment variable will generally not be identical to the actual treatment. 
By default this two-state least-squares approach estimates the compiler average treatment effect at the cut-off. By default the same six parametric and nonparametric models are being estimated, and their treatment effects, along with inferential statistics are presented in Table 3.1., with an accompanying graphical display in Figure 3.1. Covariates can be displayed via the toggle button, and if selected, the linear regression coefficient of the covariate predicting the outcome in the second stage model is displayed. 

The only addition to the sharp RDD with single assignment, is that the user may also request the intent-to-treat estimate. This estimate essentially ignores the fuzziness of the design, and simply assigns each individual the treatment that they were supposed to receive based on their value of the assignment variable and the cut-off rule. This estimate can be toggled on and will show up as an additional row in the Table, labeled "+ ITT". Also, Figure 3.1. can display the ITT model, by selecting it from the drop-down menu labeled "Model type". 

#### Sharp RDD with two assignment rules 
The RDD with multiple (in this case, two) assignment variables complicates the analysis, because numerous different estimates have been suggested in the literature. For an overview, see the paper by Wong, Steiner, and Cook (2013), which describes a variety of such designs, and estimates. The estimators implemented in this software are directly derived from the Wong et al. (2013) paper, and the Stata code that was published by Wong. The user can use the toggle buttons in the top right of Table 3.1. to display or hide various combinations of models and parametric and nonparametric estimation. We will briefly discuss each of these estimators. 

##### Centering approach
In the centering approach, the multiple assignment RDD is transformed into a single assignment RDD through the use of a centering function. In particular, the program centers all assignment variables around the cut-off value (meaning the value of the cut-off is subtracted from each score) and then the minimum score across these two deviation scores is assigned to each unit. In designs in which treatment is assigned to units above or below the cut-off (or any combination of those), sometimes the maximum is taken. Following this transformation, each unit has a single assignment score, and this score is used to compute a regular (single assignment) RDD. Note that the particular metric on which assignment variables are collected, will influence the treatment effect estimate. Wong et al. (2013) show that even standardization cannot typically mitigate this issue, and advise that the centering approach should only be used when the assignment variables share the same metric. 

All six models that were described in the previous section, along with their visualizations, are automatically computed for the centering approach. The centering approach computes what Wong et al. call the frontier average treatment effect, which is essentially a treatment effect along the whole cut-off of the multi-dimensional response surface, defined by the two assignment variables, and the outcome. The graph in Figure 3.1 can be selected under "Model Type" and then selecting "Centering". All other previously described modifications to the graph can also be implemented. 

##### Univariate approach
The univariate approach, as the name implies, focuses on treatment effects that are exclusively attributable to one of the two assignment variables. Wong et al. (2013) call these the frontier-specific effects. In a design with two assignment variables, two frontier-specific effects will be computed. To estimate these effects, the sample is divided into subsets. For example, to compute the frontier-specific effect of the first assignment variable, all units are retained that are assigned to the treatment based on this assignment variable only. Other units that are assigned to treatment based on the second assignment variable are excluded for this analysis. This will be reflected in the sample sizes that are displayed in Table 3.1. 
This subset of the data is then analyzed using a regular, single variable assignment RDD. Again, all six models that were described in the previous section will be estimated automatically. The only difference is that Table 3.1. now includes these six models for both assignment variables, which are labeled A1 and A2 in both the table and the figure.
As an aside, there is an alternative to the univariate approach, which retains all subjects, and then treats the resulting design, as a fuzzy RDD with a single assignment variable. Wong et al. (2013) demonstrated that this typically has no advantages in terms of statistical power or bias, when compared to simply excluding the mis-classified units from the analysis. 

##### Frontier approach {#frontier_approach}
The frontier approach estimates the treatment effect along the whole discontinuity frontier of the response surface. As described by Wong et al. (2013), this is performed by first estimating the response surface (e.g., through a parametric model), and then using kernel density estimates for numerical integration of the treatment effect. The frontier approach yields three different estimates of the average treatment effect - one for the whole frontier, called the average frontier effect, and one for each dimension of the frontier that is associated with one of the two assignment variables. They are simply labeled Frontier 1 and Frontier 2 in Table 3.1., and refer to the two assignment variables. All of these effects are estimated simultaneously using the full sample. This results in  typically higher statistical power than other approaches, at the expense of having to assume that the response surface is correctly modeled via a parametric model. Each of the three effects is based on a parametric model of the response surface. Currently, the software only allows for such parametric modeling of the response surface, and therefore the toggle button for the nonparametric model will be inactive. However, the user has choices regarding the particular parametric model. The three choices are "Unconstrained", "Heterogeneous effects", and "Constant Effect". The "constant effect" model is the most restrictive, as it puts an equality constraints on all discontinuities across both dimensions, and on the slopes of both assignment variables on each side of the discontinuity. That means that in this model all slopes for both assignments on both sides of the cut-offs are constrained to be equal. The fact that the treatment effect estimates are not necessarily identical across the average frontier, and frontier-specific treatment effects is due to the fact that the raw data on which the kernel density estimate for the numerical integration is based on, may not be identically distributed along the regression surface. The "heterogeneous model" relaxes some of the constraints of the previous model, in particular,  now the coefficients that code the actual discontinuity are allowed to differ, but the slopes are still constrained to be identical. Lastly, the fully "unconstrained" model allows all slopes, and all discontinuities to differ from each other. The unconstrained model is the most complex parametric model. It still imposes linearity constraints on the slopes though, which means the model could at least potentially be made even more complex, however this is not supported in the current software. Wong et al. (2013) did not provide closed form, analytic solutions for standard errors for the frontier treatment effects. Therefore we use bootstrapping to compute inferential statistics. The number of bootstrap samples can be defined in the data tab, under outcome variable. A large number of bootstrap samples may potentially take a long time to compute.
Given the complexity of the frontier approach, it is useful to also inspect the additional graphical displays in Figure 3.1. 
The software provides a three-dimensional plot of the response surface for the frontier estimates. It is accessed using the pull-down menu under "Model Type". A new pull-down is then displayed called "Model Specification", where the user can switch graphical displays of the three different parametric models. This helps to visualize the implied constraints of the various models. The user has various choices to modify the appearance of the plot. The pull-down menu labeled "View" defaults to "Perspective", which presents a three-dimensional graph of the response surface. Changing this to "Top", "Front", or "Side" changes the perspective, and typically flattens one dimension. E.g., the top view shows the response surface directly from above, therefore flattening the dimension of the outcome variable. This can sometimes be useful to visualize the cut-offs of the two assignment variables. Front and Side view can occasionally be of use, but are typically harder to interpret than the three-dimensional plot. 
The three-dimensional perspective plot can be manually rotated by the user, using the sliders that are labeled with arrows on top, and take angles as input. The graph is redrawn every time these angles are changed, and the display updates as soon as the new angle is set. It is currently not possible to interactively spin the graph. The panel button underneath the perspective view allows the user to color all surfaces that received the treatment assignment, add gridlines, and put a slight shade on all surfaces. Finally, the bottom panel labeled "Data" allows the user to overlay the raw data as points, and to scale the axes. By default the axes are rescaled so that the resulting response surface plot has axes of the same length. Using the toggle button "original scale" changes this so that the axes of the variables correspond to the actual scales, and are therefore potentially of different length. 

#### Fuzzy RDD with two assignment rules
In the case of an RDD with multiple assignments that are potentially fuzzy, the design becomes slightly more complicated, however the overall interface, and outputs are identical to the sharp counterpart of this design. All of the models that were described above, are now being estimated using the two-stage least squares approach, in which the fuzzy treatment assignment is estimated in a first step, and then the estimated treatment assignments are used for all of the models. 

## Sensitivities tab {#sensitivities_tab}
In this tab, the user may perform various checks to see how sensitive results are to particular modeling choices. The two sections 4.1. and 4.2. contain a sensitivity analysis regarding the cut-off, and the bandwidth parameter, respectively. 

Figure 4.1. shows the user how the treatment effect would have been different, if a different cut-off would have been chosen for the analysis. This cut-off is necessarily different than the one that was assumed in the original design. These tests of treatment effects at essentially wrong cut-off values are sometimes also called "placebo tests", and the assumption is that in a correct RDD, the treatment effect should only be visible at the actual, true cut-off. Hence, seeing lots of non-significant results at cut-offs away from the actual cut-off value is desirable. 
By default, the tests are not produced before the user clicks the button in the bottom right. The user can change various parameters for the analysis and plot in Figure 4.1. By default the treatment effect at the true cut-off is estimated (and shown with a vertical line in the plot), and estimates around the cut-off using a default span around it. The user may change this span, both by defining the endpoints (using the min and max boxes), and the number of estimates between these endpoints (using the steps box). More extreme endpoints, and a larger number of steps, will yield more placebo test over a wider range of values. By default a 95% confidence interval is shown around the estimates, but the user can change this by using a different value in the box at the top of the Figure. Likewise, the user can change which model is used to estimate the effects (e.g., parameteric model, nonparametric model with specific bandwidths, etc.). Simply clicking the box underneath the figure will display all possible models for this particular analysis. The models will always correspond to the ones found in the Estimates tab. The plot can be downloaded in various formats, using the buttons on the top right of the figure. 

Figure 4.2. is a robustness check for the nonparametric model only, and shows how the treatment effect changes with a different bandwidth for the nonparametric estimator. The user can change which bandwidth parameters should be probed by defining a minimum, maximium, and the number of steps between those values in the boxes underneath the graph. The optimal bandwidth is indicated by a vertical line in the plot. Ideally, the graph should show a somewhat flat line that indicates that over a wide range of bandwidth choices, the estimate does not change dramatrically. By default a 95% confidence interval is shown, however the user can change this using the box on top of the graph. The graph can be downloaded in various formats using the usual buttons in the upper right half. 

## R Code tab
In this tab, the current analysis that the user selected via the drop-down menu is translated into R code. Advanced users may copy and paste this code for further modifications directly in R. This is also useful for being able to replicate results. 

# Power page {#power_page}
Users may wish to perform a formal (frequentist) power analysis, to determine a-priori sample sizes, or to construct power curves for various sample sizes. All power analyses are performed using Monte Carlo simulations, that means that the researcher needs to define all population parameters. From these paramaters samples are drawn, and effects, and their significance are estimated. The proportion of significant results across all Monte Carlo simulations is used as an estimate of statistical power. 

Definining the population parameters is important, and often challening. We try to guide the user in this process using graphical displays. These displays will change depending on the particular design that is chosen. We begin our description with the RDD with a single assignment variable. The upper left box, called "Assignment" is where the user defines the assumed distribution of the assignment variable. The current choices are "normal" and "uniform", and the user can toggle between those using the first toggle button in the "Distribution" section. By default the assignment variable is called "A1". If the user chooses a normal distribution, a mean and a standard deviation must be provided. If the user chooses a uniform distribution, a minimum and a maximum must be provided. Depending on these entries, the graphical display of the assignment variable is updated. The display always shows a histogram with kernel density estimate, and shading that indicates group membership. 
The user may also define at which value of the assignment variable, treatment is adminstered. This typically tends to change the proportion of treated and untreated, and also affects power estimates. The user may also change the amount of "fuzzyness", which is defined as the percentage of misclassified cases around the cut-off. We allow the user to define fuzzyness on both sides of the cut-off. For example, a user may specify that 20% of the cases below the cut-off are misclassified, but only 10% of the cases on the other side of the cut-off. Finally, the user must specify the overall sample size in the box in the top right of the section. 
Clicking the plus sign in the top right of the box, allows the user to add a second assignment variable (by default called A2), and generate an RDD with two assignment variables. All parameters (including distribution, and fuzzyness) of the second variable can be modified in the same way. The plot will automatically update to show two assignment variables. 

Once the assignment variables have been defined, the user must define the parametric outcome model that relates the assignment variable, and the treatment to the outcome variable. Currently, we only allow linear models, however we allow a fully unconstrained parameteric model with respect to interactive effects. This implies that every segment of the regression surface can have a different slope. The plot will show some example data that are generated from the design, and will update depending on user input. 

In the case of a single assignment RDD, a simple scatterplot with a regression line is shown. The user can modify the treatment effect, the intercept of the regression slope, the slope on the untreated side of the cut-off, and (through the interaction effect), the difference in slopes on the other side of the cut-off. Finally, the user can define the partial $\eta^2$ that measures the effect size of the treatment. Changing this value will show up in the graph as data points lying closer to the regression line. Defining the effect size is an indirect way to change the standard error of the treatment effect. 

In the case of an RDD with two assignment variables, the parametric model is substantially more complex. It now involves treatment effects at both frontiers, and slopes that are defined for every piece of the regression surface that is being divided by the cut-off on the assignment variable. There are a total of 13 regression coefficients that need to be defined by the user. Starting with the intercept (the predicted value when both assignment variables are zero, and the assignment is to the control condition), to the two treatment effects (at the boundaries of the two assignment varaiables), to the two regression coefficients that relate the assignment variables to the outcome (while holding treatments constant at the control group). In addition, a total of 7 interaction effects are included, that define how the slopes of the assignment variables change with different values of the treatment and control values for the two cut-offs. The graphical display can help in making sense of all these coefficients. 

Once the parametric models are specified, the user can generate Monte Carlo simulations in the upper right section of the power tab. 
Table 5.1. has an execute button in the top right corner, along with a setting for the number of Monte Carlo simulations that should be conducted. By default this is set to 500, but the user can change this number. Generally speaking, higher numbers yield more precise estimates. After the simulation is finished, Table 5.1. is populated. In the case of the rows will contain the parametric linear model, and the non-parametric model with optimal bandwidth. The columns will show the number of succesful replication of the Monte Carlo simulation. Typically this will be equal to the user-specified number of iterations, but for some unsual models (e.g., lots of fuzzyness in assignment), it is possible that a simulated dataset may not yield a valid estimate. The mean and the variance of all simulated treatment effects is shown in in the next two columns. Typically, the user-defined treatment effect should be recovered. Finally, the last three columns show the estimated statistical power (1 - Type II error) for various alpha levels (.05, .01, .001). 

In the case of an RDD with two assignment variables, the same columns are displayed, but the rows now show three parametric models (centering, and two univariate models), and three non-parametric models (the same models, but with an optimal bandwidth). Other possible effects (e.g., the frontier average effect) are not computed, because they take a prohibtly long amount of time to compute. 


Figure 5.1. below allows the user to probe several sample sizes (instead of only a single sample size). The user can set the minimum and the maximum sample size, and the number of steps between these sample sizes can be set in the box at the top of the figure. By default 500 iterations are performed, but the user may increase this number for increased precision of power estimates. Finally, the user can change the display of the graph by either graphing two lines for power of the (linear) parametric model, and the optimal non-parametric model, or alterantively draw seperate lines for different alpha levels. All results of Figure 5.1. can be downloaded in various formats using the buttons in the top right of the figure. 

# About
This software was developed by [Felix Thoemmes](https://psychology.cornell.edu/felix-j-thoemmes), [Wang Liao](https://wliao229.github.io/), [Ze Jin](https://www.linkedin.com/in/ze-jin-08045b31), [Wenyu Zhang](http://zwenyu.github.io/), and [Irena Papst](https://github.com/papsti) all at Cornell University. We are happy to receive feedback! Please send it to
![](../www/fjt_email.png)


The project was supported by the [Institute of Education Sciences](https://ies.ed.gov/), [U.S. Department of Education](https://www.ed.gov/), through Grant R305D150029. The opinions expressed are those of the authors and do not represent views of the Institute or the U.S. Department of Education.
